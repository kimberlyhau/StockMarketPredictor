{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimberlyhau/StockMarketPredictor/blob/main/Final_LSTMModel_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xH_M-4_lor5",
        "outputId": "8bfbc6bd-4684-4b14-c924-9af1539d0672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import plotly.express as px\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "_dmwQThxl4ng"
      },
      "outputs": [],
      "source": [
        "#file paths\n",
        "sentiment_file = os.path.join('/content/drive/My Drive/sentiments.txt')\n",
        "sentiment_file_2 = os.path.join('/content/drive/My Drive/sentimentsNY_final.txt')\n",
        "stock_prices_folder = os.path.join('/content/drive/My Drive/stock_market_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "eWL88iEqAESc"
      },
      "outputs": [],
      "source": [
        "#open cnn and ny times sentiment data\n",
        "with open(sentiment_file, 'r') as file_obj:\n",
        "    text = file_obj.read()\n",
        "    txt_list = text.split('[')\n",
        "\n",
        "with open(sentiment_file_2, 'r') as file_obj:\n",
        "    text = file_obj.read()\n",
        "    txt_list_ny = text.split('[')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "yY6E_7gaTCSJ"
      },
      "outputs": [],
      "source": [
        "#open stock prices\n",
        "stock_data_lst = []\n",
        "for filename in os.listdir(stock_prices_folder):\n",
        "    if \".txt\" in filename:\n",
        "      #file_object = open(folder + \"/\" + filename, \"r\")\n",
        "      with open(stock_prices_folder + \"/\" + filename, 'r') as file_obj:\n",
        "        text = file_obj.read()\n",
        "        stock_data_lst.append([filename.replace('.txt', ''), re.split(',|\\n', text)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2YoZisOQipo"
      },
      "outputs": [],
      "source": [
        "#organize stock prices\n",
        "stock_data = []\n",
        "for stocks in stock_data_lst:\n",
        "  #stocks_df = pd.DataFrame(columns = ['Stock Price', 'Datestamp'])\n",
        "  stock_name = stocks[0]\n",
        "  print (stock_name)\n",
        "  datestamp_lst = []\n",
        "  stock_price_lst = []\n",
        "  if stock_name != \"SP500Data\":\n",
        "    for stock_elem in stocks[1]:\n",
        "      if re.search('[a-zA-Z]', stock_elem):\n",
        "        continue\n",
        "      elif '-' in stock_elem:\n",
        "        elems = stock_elem.split('-')\n",
        "        datestamp = int(elems[0])*10000 + int(elems[1])*100 + int(elems[2])\n",
        "        datestamp_lst.append(datestamp)\n",
        "      elif '.' in stock_elem:\n",
        "        stock_price = float(stock_elem)\n",
        "        stock_price_lst.append(stock_price)\n",
        "    stock_data.append([stock_name, pd.DataFrame(list(zip(stock_price_lst, datestamp_lst)), columns =['Stock Price', 'Datestamp'])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "t--iItF_XNkl"
      },
      "outputs": [],
      "source": [
        "#organize CNN data\n",
        "datestamp_lst = []\n",
        "sentiment_lst = []\n",
        "date_lst = []\n",
        "for i in range(1, len(txt_list)):\n",
        "  elems = re.split(\"'|-|T|:|Z|,|]| \", txt_list[i])\n",
        "  while(\"\" in elems) :\n",
        "    elems.remove(\"\")\n",
        "  date = int(elems[0])*10000 + int(elems[1])*100 + int(elems[2])\n",
        "  datestamp = int(elems[0])*10000000000 + int(elems[1])*100000000 + int(elems[2])*1000000 + int(elems[3])*10000 + int(elems[4])*100 + int(elems[5])\n",
        "  sentiment = float(elems[6])  \n",
        "  # datestamp_lst.append(datestamp) datestamp_lst\n",
        "  sentiment_lst.append(sentiment)\n",
        "  date_lst.append(date)\n",
        "sentiment_df = pd.DataFrame(list(zip(sentiment_lst, date_lst)), columns =['Sentiment', 'Date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs_cvZ5G-rB8"
      },
      "outputs": [],
      "source": [
        "#clean up cnn data\n",
        "sentiment_df_sorted = sentiment_df.sort_values('Date').set_index('Date').fillna(0)\n",
        "sentiment_df_sorted =  (sentiment_df_sorted.groupby(level='Date').mean())\n",
        "sentiment_df_sorted['null'] = [0.0]*len(sentiment_df_sorted)\n",
        "\n",
        "print (sentiment_df_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "6xwW4ppevUiv"
      },
      "outputs": [],
      "source": [
        "#organize NY times data\n",
        "datestamp_lst = []\n",
        "sentiment_lst = []\n",
        "date_lst = []\n",
        "for i in range(1, len(txt_list_ny)):\n",
        "  elems = re.split(\"'|-|T|:|Z|,|]| \", txt_list_ny[i])\n",
        "  while(\"\" in elems) :\n",
        "    elems.remove(\"\")\n",
        "  if elems[0] == 'imestamp(':\n",
        "    elems = elems[1:]\n",
        "  date = int(elems[0])*10000 + int(elems[1])*100 + int(elems[2])\n",
        "  \n",
        "  datestamp = int(elems[0])*10000000000 + int(elems[1])*100000000 + int(elems[2])*1000000  #+ int(elems[3])*10000 + int(elems[4])*100 + int(elems[5])\n",
        "  sentiment = float(elems[-1])  \n",
        "  #  datestamp_lst.append(datestamp) datestamp_lst\n",
        "  sentiment_lst.append(sentiment)\n",
        "  date_lst.append(date)\n",
        "sentiment_df_ny = pd.DataFrame(list(zip(sentiment_lst, date_lst)), columns =['Sentiment', 'Date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvdzk6huvYa_"
      },
      "outputs": [],
      "source": [
        "#organize ny times data\n",
        "sentiment_df_sorted_ny = sentiment_df_ny.sort_values('Date').set_index('Date').fillna(0)\n",
        "sentiment_df_sorted_ny =  (sentiment_df_sorted_ny.groupby(level='Date').mean())\n",
        "sentiment_df_sorted_ny['null'] = [0.0]*len(sentiment_df_sorted_ny)\n",
        "(sentiment_df_sorted_ny)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "-Ee7bqSLaYTD"
      },
      "outputs": [],
      "source": [
        "#LSTM Model\n",
        "input_features =2\n",
        "hidden = 20\n",
        "class Model(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.linear_1 = nn.Linear(input_features, hidden) #first linear layer\n",
        "        self.relu = nn.ReLU() #relu layer\n",
        "        self.lstm = nn.LSTM (hidden ,  hidden_size= hidden, num_layers = 1,) #LSTM\n",
        "        self.linear = nn.Linear(hidden,1) #output layer\n",
        " \n",
        "    def forward(self,x):\n",
        "      x = self.linear_1(x)\n",
        "      x = self.relu(x).reshape(1,1,20)\n",
        "\n",
        "      lstm_out, x = self.lstm(x) #.reshape(20,1)\n",
        "      out = self.linear(lstm_out)\n",
        "      \n",
        "      return out #.reshape(1,1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oUWpR5NadpR"
      },
      "outputs": [],
      "source": [
        "#Train Modle\n",
        "#master lists\n",
        "masterNames = []\n",
        "masterTrainAcc = []\n",
        "masterValidAcc = []\n",
        "masterTestAcc = []\n",
        "\n",
        "#Need to run for each of the 11 sectors\n",
        "for k in range(0,len(stock_data)):\n",
        "  #get stock prices\n",
        "  temp_stock =  (stock_data[k][1][252:]).reset_index(drop = True).set_index('Datestamp')\n",
        "\n",
        "\n",
        "  #store and print which one is in use\n",
        "  print (stock_data[k][0][:-9])\n",
        "  masterNames.append(stock_data[k][0][:-9])\n",
        "\n",
        "  ##prepare data\n",
        "  temp_stock =  (stock_data[k][1][252:]).reset_index(drop = True).set_index('Datestamp') #get correspondinging \n",
        "\n",
        "  #combine sentiment and stock prices  \n",
        "  sentiment_df_sorted_ny['Stock Price'] = temp_stock.copy().dropna()\n",
        "  sentiment_df_sorted_ny = sentiment_df_sorted_ny.dropna()\n",
        "  sentiment_df_sorted_ny['CNN Sentiment'] = sentiment_df_sorted[\"Sentiment\"].copy().fillna(0)\n",
        "  sentiment_df_sorted_ny['CNN null'] = sentiment_df_sorted[\"null\"].copy().fillna(0)\n",
        "\n",
        "  MasterDf = sentiment_df_sorted_ny.copy()\n",
        "  MasterDf = MasterDf.fillna(0)\n",
        "  MasterDf['Sentiment'] = MasterDf[[\"Sentiment\", \"CNN Sentiment\"]].values.tolist()\n",
        "  MasterDf['null'] = MasterDf[[\"null\", \"CNN null\"]].values.tolist()\n",
        "  MasterDf = MasterDf.fillna(0)\n",
        "\n",
        "  #Split the data into train nad test set\n",
        "  train = (MasterDf[:int(MasterDf.shape[0]*0.8)]).sample(frac=1)\n",
        "\n",
        "  X_train = torch.tensor(train['null'].reset_index(drop = True))\n",
        "  y_train = torch.tensor(train['Stock Price'].reset_index(drop = True))\n",
        "\n",
        "  test_and_val = (MasterDf[int(MasterDf.shape[0]*0.8):]).sample(frac=1)\n",
        "\n",
        "  X_test_df = test_and_val[:int(test_and_val.shape[0]*0.5)]['null']\n",
        "  y_test_df = test_and_val[:int(test_and_val.shape[0]*0.5)]['Stock Price']\n",
        "\n",
        "  X_test = torch.tensor(X_test_df.reset_index(drop = True))\n",
        "  y_test = torch.tensor(y_test_df.reset_index(drop = True))\n",
        "\n",
        "  X_val_df = test_and_val[int(test_and_val.shape[0]*0.5):]['null']\n",
        "  y_val_df = test_and_val[int(test_and_val.shape[0]*0.5):]['Stock Price']\n",
        "\n",
        "  X_val = torch.tensor(X_val_df.reset_index(drop = True))\n",
        "  y_val = torch.tensor(y_val_df.reset_index(drop = True))\n",
        "\n",
        "  #Model Params\n",
        "  epochs = 40\n",
        "  net = Model()\n",
        "  learning_rate = 0.05\n",
        "  criterion = nn.MSELoss()# BCE??\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "  #lists to store\n",
        "  train_loss_epoch = []\n",
        "  train_acc_epoch = []\n",
        "\n",
        "  valid_acc_epoch = []\n",
        "  valid_loss_epoch = []\n",
        "\n",
        "  model_y_valid = []\n",
        "  model_y = []\n",
        "\n",
        "  #training\n",
        "  for epoch in range(epochs):\n",
        "    train_acc = [] \n",
        "    train_loss =  []  \n",
        "    valid_loss = []   \n",
        "    acc_valid = [] \n",
        "\n",
        "    #update model weights\n",
        "    for i in range(0,len(X_train)):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      y = net(X_train[i].float().view(1,2))\n",
        "      loss = criterion (y, y_train[i].float().view(1,1))\n",
        "      \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_acc.append((np.abs(y.item() - y_train[i].float().view(1,1).numpy())[0][0])/y_train[i].float().view(1,1).numpy()[0][0]) #sccuracy\n",
        "      train_loss.append(loss.item()) #loss\n",
        "\n",
        "      #store model values on last epoch to plot\n",
        "      if epoch == 39:\n",
        "        model_y.append(y.item())\n",
        "\n",
        "    for j in range(0,len(X_val)):\n",
        "\n",
        "      y_2 = net(X_val[j].float().view(1,2))\n",
        "      loss_v = criterion (y_2, y_val[j].float().view(1,1))\n",
        "\n",
        "      acc_valid.append((np.abs(y_2.item() - y_val[j].float().view(1,1).numpy())[0][0])/y_val[j].float().view(1,1).numpy()[0][0])\n",
        "      valid_loss.append(loss_v.item())\n",
        "\n",
        "      #store model values on last epoch to plot\n",
        "      if epoch == 39:\n",
        "        model_y_valid.append(y_2.item())\n",
        "      \n",
        "\n",
        "    #Store average accuracy and loss for plotting\n",
        "    train_loss_epoch.append(np.mean(train_loss))\n",
        "    train_acc_epoch.append(np.mean(train_acc))      \n",
        "    valid_acc_epoch.append(np.mean(acc_valid))\n",
        "    valid_loss_epoch.append(np.mean(valid_loss))\n",
        "  \n",
        "    #print results\n",
        "    print('Epoch: %04d  Training Loss: %.2f Training Error: %.2f Valid Loss: %.2f Valid Error: %.2f' % (epoch + 1, np.mean(train_loss), np.mean(train_acc)*100,np.mean(valid_loss), np.mean(acc_valid)*100))\n",
        "\n",
        "  #store overall results to display after training everything        \n",
        "  masterTrainAcc.append(np.mean(train_acc)*100)\n",
        "  masterValidAcc.append(np.mean(acc_valid)*100)\n",
        "\n",
        "  ##Plots\n",
        "  #epoch/loss\n",
        "  plt.title(stock_data[k][0][:-9] + \" Epoch vs Loss\")\n",
        "  plt.plot(train_loss_epoch, label=\"Train\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  #epoch/errror\n",
        "  plt.title(stock_data[k][0][:-9]+\" Epoch vs Error\")\n",
        "  plt.plot(train_acc_epoch, label=\"Train\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Error (%)\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  #Train results\n",
        "  plt.title(\"Train: \"+stock_data[k][0][:-9]+\" Model Close Price vs True Close Price\")\n",
        "  plt.plot([i for i in range(0,len(y_train.detach().numpy()))], y_train.detach().numpy()) #, label=\"Train Price\")\n",
        "  plt.plot([i for i in range(0,len(y_train.detach().numpy()))], model_y, label=\"Model Price\")\n",
        "  plt.xticks([i for i in range(0,len(y_train.detach().numpy()), 40)],pd.to_datetime(train.index.sort_values (), format = \"%Y%m%d\")[::40],rotation = 45)\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Stock Price\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  #Validation results\n",
        "  plt.title(\"Valid :\" +stock_data[k][0][:-9]+\" Model Close Price vs True Close Price\")\n",
        "  plt.plot([i for i in range(0,len(y_val.detach().numpy()))], y_val.detach().numpy(), label=\"Valid Close Price\")\n",
        "  plt.plot([i for i in range(0,len(y_val.detach().numpy()))], model_y_valid, label=\"Model Close Price\")\n",
        "  plt.xticks([i for i in range(0,len(y_val.detach().numpy()),10)],pd.to_datetime(X_val_df.index.sort_values (), format = \"%Y%m%d\")[::10],rotation = 45)\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Stock Price\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  ##test\n",
        "  #Get test results\n",
        "  model_y = []\n",
        "  test_acc = []\n",
        "  test_loss = []\n",
        "\n",
        "  for i in range(0,len(X_test)):\n",
        " \n",
        "      y = net(X_test[i].float().view(1,2))\n",
        "      loss = criterion (y, y_test[i].float().view(1,1))\n",
        "\n",
        "  \n",
        "      test_acc.append((np.abs(y.item() - y_test[i].float().view(1,1).numpy())[0][0])/y_test[i].float().view(1,1).numpy()[0][0])\n",
        "      test_loss.append(loss.item())\n",
        "\n",
        "      model_y.append(y.item())\n",
        "\n",
        "  \n",
        "  #PLot Test Results   \n",
        "  plt.title(\"Test: \"+stock_data[k][0][:-9]+\" Model Close Price vs True Close Price\")\n",
        "  plt.plot([i for i in range(0,len(y_test.detach().numpy()[:len(y_test)-1]))], y_test.detach().numpy()[:len(y_test)-1], label=\"Train Price\")\n",
        "  plt.plot([i for i in range(0,len(y_test.detach().numpy()[:len(y_test)-1]))], model_y[1:], label=\"Model Price\")\n",
        "  plt.xticks([i for i in range(0,len(y_test.detach().numpy()[:len(y_test)-1]))][::5],pd.to_datetime(X_test_df[1:].index.sort_values(), format = \"%Y%m%d\")[::5],rotation = 45)\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Stock Price\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  masterTestAcc.append(np.mean(test_acc)*100)\n",
        "\n",
        " \n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSpurWAIq-Do"
      },
      "outputs": [],
      "source": [
        "result =  (pd.DataFrame({'Sector':masterNames, 'Training Error (%)':masterTrainAcc,'Test Error (%)':masterTestAcc}))\n",
        "display (result)\n",
        "print (result.mean(axis=0))\n",
        "#'Validation Accuracy':masterValidAcc,\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final_LSTMModel_Predictor.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}